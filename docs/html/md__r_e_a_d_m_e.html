<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>SuperSLAM: ORB-SLAM2</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">SuperSLAM
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">ORB-SLAM2 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><b>ORB-SLAM2 Authors:</b> <a href="http://webdiis.unizar.es/~raulmur/">Raul Mur-Artal</a>, <a href="http://webdiis.unizar.es/~jdtardos/">Juan D. Tardos</a>, <a href="http://webdiis.unizar.es/~josemari/">J. M. M. Montiel</a> and <a href="http://doriangalvez.com/">Dorian Galvez-Lopez</a> (<a href="https://github.com/dorian3d/DBoW2">DBoW2</a>). The original implementation can be found <a href="https://github.com/raulmur/ORB_SLAM2.git">here</a>.</p>
<h1><a class="anchor" id="autotoc_md6"></a>
ORB-SLAM2 ROS node</h1>
<p>This is the ROS implementation of the ORB-SLAM2 real-time SLAM library for <b>Monocular</b>, <b>Stereo</b> and <b>RGB-D</b> cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. This implementation removes the Pangolin dependency, and the original viewer. All data I/O is handled via ROS topics. For vizualization you can use RViz. This repository is maintained by <a href="http://lennarthaller.de">Lennart Haller</a> on behalf of <a href="http://appliedai.de">appliedAI</a>. </p>
<h2><a class="anchor" id="autotoc_md7"></a>
Features</h2>
<ul>
<li>Full ROS compatibility</li>
<li>Supports a lot of cameras out of the box, such as the Intel RealSense family. See the run section for a list</li>
<li>Data I/O via ROS topics</li>
<li>Parameters can be set with the rqt_reconfigure gui during runtime</li>
<li>Very quick startup through considerably sped up vocab file loading</li>
<li>Full Map save and load functionality based on <a href="https://github.com/raulmur/ORB_SLAM2/pull/381">this PR</a>.</li>
<li>Loading of all parameters via launch file</li>
<li>Supports loading cam parameters from cam_info topic</li>
</ul>
<h3><a class="anchor" id="autotoc_md8"></a>
Related Publications:</h3>
<p>[Monocular] Raúl Mur-Artal, J. M. M. Montiel and Juan D. Tardós. <b>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</b>. <em>IEEE Transactions on Robotics,</em> vol. 31, no. 5, pp. 1147-1163, 2015. (<b>2015 IEEE Transactions on Robotics Best Paper Award</b>). <b><a href="http://webdiis.unizar.es/~raulmur/MurMontielTardosTRO15.pdf">PDF</a></b>.</p>
<p>[Stereo and RGB-D] Raúl Mur-Artal and Juan D. Tardós. <b>ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras</b>. <em>IEEE Transactions on Robotics,</em> vol. 33, no. 5, pp. 1255-1262, 2017. <b><a href="https://128.84.21.199/pdf/1610.06475.pdf">PDF</a></b>.</p>
<p>[<a class="el" href="namespace_d_bo_w2.html">DBoW2</a> Place Recognizer] Dorian Gálvez-López and Juan D. Tardós. <b>Bags of Binary Words for Fast Place Recognition in Image Sequences</b>. <em>IEEE Transactions on Robotics,</em> vol. 28, no. 5, pp. 1188-1197, 2012. <b><a href="http://doriangalvez.com/php/dl.php?dlp=GalvezTRO12.pdf">PDF</a></b></p>
<h1><a class="anchor" id="autotoc_md9"></a>
1. License</h1>
<p>ORB-SLAM2 is released under a <a href="https://github.com/aaide/ORB_SLAM2_ROS/blob/master/License-gpl.txt">GPLv3 license</a>. For a list of all code/library dependencies (and associated licenses), please see <a href="https://github.com/aaide/ORB_SLAM2_ROS/blob/master/Dependencies.md">Dependencies.md</a>.</p>
<p>For a closed-source version of ORB-SLAM2 for commercial purposes, please contact the authors: orbslam (at) unizar (dot) es.</p>
<p>If you use ORB-SLAM2 (Monocular) in an academic work, please cite: </p><pre class="fragment">@article{murTRO2015,
  title={{ORB-SLAM}: a Versatile and Accurate Monocular {SLAM} System},
  author={Mur-Artal, Ra\'ul, Montiel, J. M. M. and Tard\'os, Juan D.},
  journal={IEEE Transactions on Robotics},
  volume={31},
  number={5},
  pages={1147--1163},
  doi = {10.1109/TRO.2015.2463671},
  year={2015}
 }
</pre><p>if you use ORB-SLAM2 (Stereo or RGB-D) in an academic work, please cite: </p><pre class="fragment">@article{murORB2,
  title={{ORB-SLAM2}: an Open-Source {SLAM} System for Monocular, Stereo and {RGB-D} Cameras},
  author={Mur-Artal, Ra\'ul and Tard\'os, Juan D.},
  journal={IEEE Transactions on Robotics},
  volume={33},
  number={5},
  pages={1255--1262},
  doi = {10.1109/TRO.2017.2705103},
  year={2017}
 }
</pre><h1><a class="anchor" id="autotoc_md10"></a>
2. Building orb_slam2_ros</h1>
<p>We have tested the library in <b>Ubuntu 16.04</b> with <b>ROS Kinetic</b> and <b>Ubuntu 18.04</b> with <b>ROS Melodic</b>. A powerful computer (e.g. i7) will ensure real-time performance and provide more stable and accurate results. A C++11 compiler is needed.</p>
<h2><a class="anchor" id="autotoc_md11"></a>
Getting the code</h2>
<p>Clone the repository into your catkin workspace: </p><div class="fragment"><div class="line">git clone https://github.com/appliedAI-Initiative/orb_slam_2_ros.git</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md12"></a>
ROS</h2>
<p>This ROS node requires catkin_make_isolated or catkin build to build. This package depends on a number of other ROS packages which ship with the default installation of ROS. If they are not installed use <a href="http://wiki.ros.org/rosdep">rosdep</a> to install them. In your catkin folder run </p><div class="fragment"><div class="line">sudo rosdep init</div>
<div class="line">rosdep update</div>
<div class="line">rosdep install --from-paths src --ignore-src -r -y</div>
</div><!-- fragment --><p>to install all dependencies for all packages. If you already initialized rosdep you get a warning which you can ignore.</p>
<h2><a class="anchor" id="autotoc_md13"></a>
Eigen3</h2>
<p>Required by <a class="el" href="namespaceg2o.html">g2o</a>. Download and install instructions can be found <a href="http://eigen.tuxfamily.org">here</a>. Otherwise Eigen can be installed as a binary with: </p><div class="fragment"><div class="line">sudo apt install libeigen3-dev</div>
</div><!-- fragment --><p><b>Required at least Eigen 3.1.0</b>.</p>
<h2><a class="anchor" id="autotoc_md14"></a>
Building</h2>
<p>To build the node run </p><div class="fragment"><div class="line">catkin build</div>
</div><!-- fragment --><p>in your catkin folder.</p>
<h1><a class="anchor" id="autotoc_md15"></a>
3. Configuration</h1>
<h2><a class="anchor" id="autotoc_md16"></a>
Vocab file</h2>
<p>To run the algorithm expects both a vocabulary file (see the paper) which ships with this repository.</p>
<h1><a class="anchor" id="autotoc_md17"></a>
Config</h1>
<p>The config files for camera calibration and tracking hyper paramters from the original implementation are replaced with ros paramters which get set from a launch file.</p>
<h2><a class="anchor" id="autotoc_md18"></a>
ROS parameters, topics and services</h2>
<h3><a class="anchor" id="autotoc_md19"></a>
Parameters</h3>
<p>There are three types of parameters right now: static- and dynamic ros parameters and camera settings. The static parameters are send to the ROS parameter server at startup and are not supposed to change. They are set in the launch files which are located at ros/launch. The parameters are:</p>
<ul>
<li><b>load_map</b>: Bool. If set to true, the node will try to load the map provided with map_file at startup.</li>
<li><b>map_file</b>: String. The name of the file the map is loaded from.</li>
<li><b>voc_file</b>: String. The location of config vocanulary file mentioned above.</li>
<li><b>publish_pointcloud</b>: Bool. If the pointcloud containing all key points (the map) should be published.</li>
<li><b>publish_pose</b>: Bool. If a PoseStamped message should be published. Even if this is false the tf will still be published.</li>
<li><b>pointcloud_frame_id</b>: String. The Frame id of the Pointcloud/map.</li>
<li><b>camera_frame_id</b>: String. The Frame id of the camera position.</li>
<li><b>load_calibration_from_cam</b>: Bool. If true, camera calibration is read from a <code>camera_info</code> topic. Otherwise it is read from launch file params.</li>
</ul>
<p>Dynamic parameters can be changed at runtime. Either by updating them directly via the command line or by using <a href="http://wiki.ros.org/rqt_reconfigure">rqt_reconfigure</a> which is the recommended way. The parameters are:</p>
<ul>
<li><b>localize_only</b>: Bool. Toggle from/to only localization. The SLAM will then no longer add no new points to the map.</li>
<li><b>reset_map</b>: Bool. Set to true to erase the map and start new. After reset the parameter will automatically update back to false.</li>
<li><b>min_num_kf_in_map</b>: Int. Number of key frames a map has to have to not get reset after tracking is lost.</li>
<li><b>min_observations_for_ros_map</b>: Int. Number of minimal observations a key point must have to be published in the point cloud. This doesn't influence the behavior of the SLAM itself at all.</li>
</ul>
<p>Finally, the intrinsic camera calibration parameters along with some hyperparameters can be found in the specific yaml files in orb_slam2/config.</p>
<h3><a class="anchor" id="autotoc_md20"></a>
Published topics</h3>
<p>The following topics are being published and subscribed to by the nodes:</p><ul>
<li>All nodes publish (given the settings) a <b>PointCloud2</b> containing all key points of the map.</li>
<li>Also all nodes publish (given the settings) a <b>PoseStamped</b> with the current pose of the camera.</li>
<li>Live <b>image</b> from the camera containing the currently found key points and a status text.</li>
<li>A <b>tf</b> from the pointcloud frame id to the camera frame id (the position).</li>
</ul>
<h3><a class="anchor" id="autotoc_md21"></a>
Subscribed topics</h3>
<ul>
<li>The mono node subscribes to:<ul>
<li>**/camera/image_raw** for the input image</li>
<li>**/camera/camera_info** for camera calibration (if <code>load_calibration_from_cam</code>) is <code>true</code></li>
</ul>
</li>
<li>The RGBD node subscribes to:<ul>
<li>**/camera/rgb/image_raw** for the RGB image</li>
<li>**/camera/depth_registered/image_raw** for the depth information</li>
<li>**/camera/rgb/camera_info** for camera calibration (if <code>load_calibration_from_cam</code>) is <code>true</code></li>
</ul>
</li>
<li>The stereo node subscribes to:<ul>
<li><b>image_left/image_color_rect</b> and</li>
<li><b>image_right/image_color_rect</b> for corresponding images</li>
<li><b>image_left/camera_info</b> for camera calibration (if <code>load_calibration_from_cam</code>) is <code>true</code></li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md22"></a>
4. Services</h1>
<p>All nodes offer the possibility to save the map via the service node_type/save_map. So the save_map services are:</p><ul>
<li>**/orb_slam2_rgbd/save_map**</li>
<li>**/orb_slam2_mono/save_map**</li>
<li>**/orb_slam2_stereo/save_map**</li>
</ul>
<p>The save_map service expects the name of the file the map should be saved at as input.</p>
<p>At the moment, while the save to file takes place, the SLAM is inactive.</p>
<h1><a class="anchor" id="autotoc_md23"></a>
5. Run</h1>
<p>After sourcing your setup bash using </p><div class="fragment"><div class="line">source devel/setup.bash</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md24"></a>
Suported cameras</h2>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Camera </th><th class="markdownTableHeadNone">Mono </th><th class="markdownTableHeadNone">Stereo </th><th class="markdownTableHeadNone">RGBD  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Intel RealSense r200 </td><td class="markdownTableBodyNone"><code>roslaunch orb_slam2_ros orb_slam2_r200_mono.launch</code> </td><td class="markdownTableBodyNone"><code>roslaunch orb_slam2_ros orb_slam2_r200_stereo.launch</code> </td><td class="markdownTableBodyNone"><code>roslaunch orb_slam2_ros orb_slam2_r200_rgbd.launch</code>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Intel RealSense d435 </td><td class="markdownTableBodyNone"><code>roslaunch orb_slam2_ros orb_slam2_d435_mono.launch</code> </td><td class="markdownTableBodyNone">- </td><td class="markdownTableBodyNone"><code>roslaunch orb_slam2_ros orb_slam2_d435_rgbd.launch</code>  </td></tr>
</table>
<p>| Mynteye S | <code>roslaunch orb_slam2_ros orb_slam2_mynteye_s_mono.launch</code> | <code>roslaunch orb_slam2_ros orb_slam2_mynteye_s_stereo.launch</code> | - | | | | |</p>
<p>Use the command from the corresponding cell for your camera to launch orb_slam2_ros with the right parameters for your setup.</p>
<h1><a class="anchor" id="autotoc_md25"></a>
6. Docker</h1>
<p>An easy way is to use orb_slam2_ros with Docker. This repository ships with a Dockerfile based on ROS kinetic. The container includes orb_slam2_ros as well as the Intel RealSense package for quick testing and data collection.</p>
<h1><a class="anchor" id="autotoc_md26"></a>
7. FAQ</h1>
<p>Here are some answers to frequently asked questions. </p>
<h3><a class="anchor" id="autotoc_md27"></a>
How to save the map</h3>
<p>To save the map with a simple command line command run one the commands (matching to your node running): </p><div class="fragment"><div class="line">rosservice call /orb_slam2_rgbd/save_map map.bin</div>
<div class="line">rosservice call /orb_slam2_stereo/save_map map.bin</div>
<div class="line">rosservice call /orb_slam2_mono/save_map map.bin</div>
</div><!-- fragment --><p>You can replace "map.bin" with any file name you want. The file will be saved at ROS_HOME which is by default ~/.ros</p>
<p><b>Note</b> that you need to source your catkin workspace in your terminal in order for the services to become available.</p>
<h3><a class="anchor" id="autotoc_md28"></a>
Using a new / different camera</h3>
<p>You can use this SLAM with almost any mono, stereo or RGBD cam you want. In order to use this with a different camera you need to supply a set of paramters to the algorithm. They are loaded from a launch file from the ros/launch folder. 1) You need the <b>camera intrinsics and some configurations</b>. <a href="https://docs.opencv.org/3.1.0/dc/dbb/tutorial_py_calibration.html">Here</a> you can read about what the camera calibration parameters mean. Use <a href="http://wiki.ros.org/camera_calibration">this</a> ros node to obtain them for your camera. If you use a stereo or RGBD cam in addition to the calibration and resolution you also need to adjust three other parameters: Camera.bf, ThDepth and DepthMapFactor. 2) <b>The ros launch file</b> which is at ros/launch needs to have the correct topics to subscribe to from the new camera. <b>NOTE</b> If your camera supports this, orb_slam_2_ros can subscribe to the camera_info topic and read the camera calibration parameters from there.</p>
<h3><a class="anchor" id="autotoc_md29"></a>
Problem running the realsense node</h3>
<p>The node for the RealSense fails to launch when running </p><div class="fragment"><div class="line">roslaunch realsense2_camera rs_rgbd.launch</div>
</div><!-- fragment --><p>to get the depth stream. <b>Solution:</b> install the rgbd-launch package with the command (make sure to adjust the ROS distro if needed): </p><div class="fragment"><div class="line">sudo apt install ros-melodic-rgbd-launch</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
